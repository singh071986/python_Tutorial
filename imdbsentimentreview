class Imdbreview:
    def __init__(self):
        pass

    def imdbreviewsentiments(self):
            ############### Sentiment Analysis using LSTM #################
            import pandas as pd
            import numpy as np
            from sklearn.model_selection import train_test_split
            from sklearn.preprocessing import LabelEncoder
            import tensorflow as tf
            from tensorflow.keras.preprocessing.text import Tokenizer
            from tensorflow.keras.preprocessing.sequence import pad_sequences
            from tensorflow.keras import layers, models
            import matplotlib.pyplot as plt


            CSV_PATH = "/Users/Yuvaan/Downloads/dataset/IMDBDataset.csv"  # download from the given link and save with this name
            TEXT_COLUMN = "review"
            LABEL_COLUMN = "sentiment"
            BATCH_SIZE = 50
            EPOCHS = 10

            df = pd.read_csv(CSV_PATH,engine='python', on_bad_lines='skip')
            print("Columns:", df.columns.tolist())
            print(df.head())
            # Drop rows with missing text or labels
            df = df.dropna()
            #label_counts = df[LABEL_COLUMN].value_counts()

            reviews = df[TEXT_COLUMN].astype(str).values
            labels = df[LABEL_COLUMN].values

            tokenizer = Tokenizer()
            tokenizer.fit_on_texts(reviews)
            sequences = tokenizer.texts_to_sequences(reviews)
            vocab_size = len(tokenizer.word_index) + 1
            print("\nVocabulary size:", vocab_size)
            max_seq_len = max(len(seq) for seq in sequences)
            print("Maximum sequence length:", max_seq_len)
            X_padded = pad_sequences(sequences, maxlen=max_seq_len, padding="post",truncating="post")  # Pad sequences to the same length
            print(X_padded)

            label_encoder = LabelEncoder()
            y = label_encoder.fit_transform(labels)
            x_train, x_test, y_train, y_test = train_test_split(X_padded, y, test_size=0.3, random_state=42, stratify=y)
            print("\nTrain size:", x_train.shape[0])
            print("Test size:", x_test.shape[0])

            # Build the LSTM model

            EMBEDDING_DIM = 50  # try 50, 100, 200 depending on dataset size and compute
            model_sigmoid = models.Sequential([
                    layers.Embedding(input_dim=vocab_size, output_dim=EMBEDDING_DIM, input_length=max_seq_len),
                    layers.LSTM(128, return_sequences=True),
                    layers.LSTM(64),
                    layers.Dense(100, activation="relu"),
                    layers.Dropout(0.5),
                    layers.Dense(1, activation="sigmoid")  # single unit
            ])
            model_sigmoid.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])


            model_sigmoid.summary()

            model_sigmoid.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
            history = model_sigmoid.fit(x_train, y_train, batch_size=BATCH_SIZE, epochs=EPOCHS, validation_split=0.1, verbose=1)


            # test_loss, test_acc = model_sigmoid.evaluate(x_test, y_test, verbose=0)
            results = model_sigmoid.evaluate(x_test, y_test, verbose=0, return_dict=True)
            test_loss = results['loss']
            test_acc = results['accuracy']

            print( f"\nTest accuracy: {test_acc:.4f}" )

            # Plot training & validation accuracy values
            plt.plot(history.history['accuracy'], label='train_accuracy')
            plt.plot(history.history['val_accuracy'], label='val_accuracy')
            plt.xlabel('Epoch')
            plt.ylabel('Accuracy')
            plt.legend()
            plt.show()


bc = Imdbreview()
print(bc.imdbreviewsentiments())
